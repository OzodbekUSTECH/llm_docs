import hashlib
import uuid
from pathlib import Path
from typing import List, Dict, Any
from fastapi import UploadFile
from sentence_transformers import SentenceTransformer

from app.entities.documents import Document
from app.repositories.documents import DocumentsRepository
from app.repositories.qdrant_embeddings import QdrantEmbeddingsRepository
from app.repositories.uow import UnitOfWork
from app.utils.enums import DocumentStatus
from app.exceptions.app_error import AppError
from qdrant_client import AsyncQdrantClient

from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker



class CreateDocumentInteractor:
    def __init__(
        self,
        uow: UnitOfWork,
        documents_repository: DocumentsRepository,
        qdrant_embeddings_repository: QdrantEmbeddingsRepository,
        sentence_transformer: SentenceTransformer,
        qdrant_client: AsyncQdrantClient,
        document_converter: DocumentConverter,
        docling_chunker: HybridChunker
    ):
        self.uow = uow
        self.documents_repository = documents_repository
        self.qdrant_embeddings_repository = qdrant_embeddings_repository
        self.sentence_transformer = sentence_transformer
        self.qdrant_client = qdrant_client
        self.storage_dir = Path("storage/documents")
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.document_converter = document_converter
        self.docling_chunker = docling_chunker

    async def execute(self, file: UploadFile) -> Document:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Docling –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
        Docling –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        # 1. –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å
        file_bytes = await file.read()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
        if not file_bytes or len(file_bytes) == 0:
            raise AppError(status_code=400, message="–§–∞–π–ª –ø—É—Å—Ç–æ–π")

        # 2. –°—á–∏—Ç–∞–µ–º —Ö—ç—à
        file_hash = hashlib.sha256(file_bytes).hexdigest()

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —Ç–∞–∫–æ–≥–æ —Ñ–∞–π–ª–∞ —É–∂–µ –≤ –ë–î
        existing = await self.documents_repository.get_one(
            where=[Document.file_hash == file_hash]
        )
        if existing:
            raise AppError(status_code=400, message="–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
        ext = Path(file.filename).suffix
        id = uuid.uuid4()
        stored_filename = f"{id}{ext}"
        file_path = self.storage_dir / stored_filename
        with open(file_path, "wb") as f:
            f.write(file_bytes)

        # 5. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é Docling
        try:
            print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file.filename}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            result = self.document_converter.convert(str(file_path))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            structured_content = self._extract_structured_content(result)
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Ç–∞–±–ª–∏—Ü
            # strict_text=False –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ—á—å –º–∞–∫—Å–∏–º—É–º —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü
            markdown_content = result.document.export_to_markdown(strict_text=False)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
            tables = self._extract_tables_from_docling_result(result)
            
            print(f"‚úÖ Docling –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω:")
            print(f"   - Markdown –∫–æ–Ω—Ç–µ–Ω—Ç: {len(markdown_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   - –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(structured_content.get('elements', []))}")
            print(f"   - –¢–∞–±–ª–∏—Ü: {len(tables)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ markdown –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            if markdown_content:
                preview = markdown_content[:500].replace('\n', '\\n')
                print(f"   üìÑ –ü—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {preview}...")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å Docling: {e}")
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
            if file_path.exists():
                file_path.unlink()
            
            error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {ext}. "
            if ext.lower() == '.pdf':
                error_msg += "PDF –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –±—ã—Ç—å –∑–∞—â–∏—â—ë–Ω –ø–∞—Ä–æ–ª–µ–º. "
                error_msg += "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç."
            else:
                error_msg += "–§–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –≤ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."
            raise AppError(status_code=400, message=error_msg)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç
        if not markdown_content or not markdown_content.strip():
            if file_path.exists():
                file_path.unlink()
            raise AppError(
                status_code=400, 
                message="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞. –§–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
            )
        
        # –î–ª—è PDF —Å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –≤—ã–≤–æ–¥–∏–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        if ext.lower() == '.pdf' and len(markdown_content.strip()) < 100:
            print(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç PDF –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π ({len(markdown_content)} —Å–∏–º–≤–æ–ª–æ–≤). "
                  f"–í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.")

        print(f"üìÑ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ:")
        print(f"   - –¢–∏–ø: {file.content_type}")
        print(f"   - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {ext}")
        print(f"   - –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(markdown_content)} —Å–∏–º–≤–æ–ª–æ–≤")

        # 6. –°–æ–∑–¥–∞—ë–º Document —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        document = Document(
            id=id,
            filename=stored_filename,
            original_filename=file.filename,
            file_path=str(file_path),
            content_type=file.content_type or "application/octet-stream",
            file_hash=file_hash,
            status=DocumentStatus.COMPLETED,
            content=markdown_content,
            tables=tables,
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ doc_metadata
            doc_metadata={
                "structured_content": structured_content,
                "parsing_method": "docling",
                "has_tables": len(tables) > 0
            }
        )
        await self.documents_repository.create(document)

        # 7. –î–µ–ª–∏–º –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é Docling chunker
        chunks = self._chunk_with_docling(result)
        
        print(f"üì¶ Chunking —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        if chunks:
            avg_size = sum(len(chunk) for chunk in chunks) / len(chunks)
            print(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   - –†–∞–∑–º–µ—Ä—ã –ø–µ—Ä–≤—ã—Ö 5: {[len(c) for c in chunks[:5]]}")

        # 8. –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –¥–ª—è E5 –º–æ–¥–µ–ª–∏
        if chunks and any(chunk.strip() for chunk in chunks):
            print(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
            
            # –í–ê–ñ–ù–û: E5 –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å "passage: " –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            # –≠—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ 10-15%!
            chunks_with_prefix = ["passage: " + chunk for chunk in chunks]
            
            embeddings = self.sentence_transformer.encode(
                chunks_with_prefix,
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=32,
                normalize_embeddings=True  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            )
            
            print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {embeddings.shape}")
            print(f"   üìä –° –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 'passage:' –¥–ª—è E5 –º–æ–¥–µ–ª–∏")
            
            # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ Qdrant —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            await self.qdrant_embeddings_repository.bulk_create_embeddings(
                document_id=str(document.id),
                chunks=chunks,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ë–ï–ó –ø—Ä–µ—Ñ–∏–∫—Å–∞
                embeddings=embeddings.tolist(),
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                metadata={
                    "filename": document.original_filename,
                    "content_type": file.content_type,
                    "has_tables": len(tables) > 0,
                }
            )
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Qdrant —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Qdrant")

        # 10. –ö–æ–º–º–∏—Ç–∏–º UoW
        await self.uow.commit()
        print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω")

        return document

    def _extract_structured_content(self, result) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Docling.
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã.
        
        –í–ê–ñ–ù–û: –î–ª—è —Ç–∞–±–ª–∏—Ü –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞–∫—Å–∏–º—É–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.
        """
        structured = {
            "elements": [],
            "metadata": {}
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–∞ iterate_items
            if not hasattr(result.document, 'iterate_items'):
                print(f"‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥–∞ iterate_items")
                return structured
            
            # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
            element_count = 0
            elements_by_type = {}
            
            for item in result.document.iterate_items():
                element_count += 1
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø —ç–ª–µ–º–µ–Ω—Ç–∞
                item_type = getattr(item, 'label', 'paragraph')
                elements_by_type[item_type] = elements_by_type.get(item_type, 0) + 1
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
                text = ""
                
                # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π –∞—Ç—Ä–∏–±—É—Ç text
                if hasattr(item, 'text') and item.text:
                    text = item.text
                
                # –°–ø–æ—Å–æ–± 2: –¥–ª—è —Ç–∞–±–ª–∏—Ü –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ export
                elif item_type == 'table' and hasattr(item, 'export_to_markdown'):
                    try:
                        text = item.export_to_markdown()
                    except:
                        pass
                
                # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ str() –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–∏–ø–æ–≤
                if not text and hasattr(item, '__str__'):
                    try:
                        text_candidate = str(item)
                        if text_candidate and len(text_candidate) < 10000:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
                            text = text_candidate
                    except:
                        pass
                
                element = {
                    "type": item_type,
                    "text": text,
                    "level": getattr(item, 'level', None),
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å (–Ω–æ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∏—Ö!)
                if hasattr(item, 'bbox'):
                    bbox = item.bbox
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º bbox –≤ dict –µ—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç
                    if hasattr(bbox, '__dict__'):
                        element["bbox"] = {
                            'l': getattr(bbox, 'l', 0),
                            't': getattr(bbox, 't', 0),
                            'r': getattr(bbox, 'r', 0),
                            'b': getattr(bbox, 'b', 0),
                        }
                    else:
                        element["bbox"] = bbox
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
                if text.strip():
                    structured["elements"].append(element)
            
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {element_count}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(structured['elements'])}")
            print(f"üìä –¢–∏–ø—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {elements_by_type}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –µ—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç)
            if hasattr(result.document, 'metadata'):
                metadata = result.document.metadata
                if hasattr(metadata, '__dict__'):
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç –≤ dict
                    structured["metadata"] = {
                        k: v for k, v in metadata.__dict__.items() 
                        if not k.startswith('_') and isinstance(v, (str, int, float, bool, type(None)))
                    }
                elif isinstance(metadata, dict):
                    structured["metadata"] = metadata
                else:
                    structured["metadata"] = {}
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
            import traceback
            traceback.print_exc()
        
        return structured

    def _extract_tables_from_docling_result(self, result) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Docling —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
        –ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –º–∞–∫—Å–∏–º—É–º —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.
        """
        tables = []
        table_count = 0
        
        try:
            for item in result.document.iterate_items():
                if hasattr(item, 'label') and item.label == 'table':
                    table_count += 1
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
                    table_text = ""
                    
                    # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π —Ç–µ–∫—Å—Ç
                    if hasattr(item, 'text') and item.text:
                        table_text = item.text
                    
                    # –°–ø–æ—Å–æ–± 2: export_to_markdown
                    if not table_text and hasattr(item, 'export_to_markdown'):
                        try:
                            table_text = item.export_to_markdown()
                        except:
                            pass
                    
                    # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ data –µ—Å–ª–∏ –µ—Å—Ç—å
                    if not table_text and hasattr(item, 'data'):
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å data –≤ —Ç–µ–∫—Å—Ç
                            data = item.data
                            if isinstance(data, (list, dict)):
                                table_text = str(data)
                        except:
                            pass
                    
                    table_data = {
                        'text': table_text,
                        'label': item.label,
                        'rows': [],
                        'metadata': {}
                    }
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã (–Ω–æ –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –æ–±—ä–µ–∫—Ç—ã!)
                    # data –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã
                    # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º text –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω
                    
                    if hasattr(item, 'bbox'):
                        bbox = item.bbox
                        if hasattr(bbox, '__dict__'):
                            table_data['bbox'] = {
                                'l': getattr(bbox, 'l', 0),
                                't': getattr(bbox, 't', 0),
                                'r': getattr(bbox, 'r', 0),
                                'b': getattr(bbox, 'b', 0),
                            }
                        else:
                            table_data['bbox'] = bbox
                    
                    if table_text.strip():
                        tables.append(table_data)
                        print(f"   ‚úì –¢–∞–±–ª–∏—Ü–∞ {table_count}: {len(table_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    else:
                        print(f"   ‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ {table_count}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
                        
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")
            import traceback
            traceback.print_exc()
        
        if table_count > 0:
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {table_count}, –∏–∑–≤–ª–µ—á–µ–Ω–æ —Å —Ç–µ–∫—Å—Ç–æ–º: {len(tables)}")
        
        return tables

    def _chunk_with_docling(self, docling_result) -> List[str]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é Docling HybridChunker.
        
        HybridChunker:
        1. –£–≤–∞–∂–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–µ —Ä–∞–∑—Ä—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏)
        2. –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ contextualize()
        3. –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã, –∞ –Ω–µ —Å–∏–º–≤–æ–ª—ã
        4. –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏
        
        """
       
        try:
            if not docling_result or not hasattr(docling_result, 'document'):
                raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç Docling")
            
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º Docling HybridChunker...")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä —á–∞–Ω–∫–æ–≤
            chunk_iter = self.docling_chunker.chunk(dl_doc=docling_result.document)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
            chunks = []
            chunk_stats = {
                'total': 0,
                'with_context': 0,
                'min_size': float('inf'),
                'max_size': 0,
            }
            
            for i, chunk in enumerate(chunk_iter):
                if not hasattr(chunk, 'text') or not chunk.text.strip():
                    continue
                
                chunk_stats['total'] += 1
                
                # –ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢: –∏—Å–ø–æ–ª—å–∑—É–µ–º contextualize() –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                # –≠—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤ –∫ —á–∞–Ω–∫—É –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
                enriched_text = self.docling_chunker.contextualize(chunk=chunk)
                
                # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω (—Ç–µ–∫—Å—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è), –æ—Ç–º–µ—á–∞–µ–º —ç—Ç–æ
                if enriched_text != chunk.text:
                    chunk_stats['with_context'] += 1
                
                chunks.append(enriched_text.strip())
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
                text_len = len(enriched_text)
                chunk_stats['min_size'] = min(chunk_stats['min_size'], text_len)
                chunk_stats['max_size'] = max(chunk_stats['max_size'], text_len)
            
            if not chunks:
                raise ValueError("Docling chunker –Ω–µ —Å–æ–∑–¥–∞–ª —á–∞–Ω–∫–∏")
            
            # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            avg_size = sum(len(c) for c in chunks) / len(chunks)
            print(f"‚úÖ Docling HybridChunker:")
            print(f"   ‚îú‚îÄ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {chunk_stats['total']}")
            print(f"   ‚îú‚îÄ –° –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {chunk_stats['with_context']} ({chunk_stats['with_context']/chunk_stats['total']*100:.1f}%)")
            print(f"   ‚îú‚îÄ –†–∞–∑–º–µ—Ä—ã: min={chunk_stats['min_size']}, avg={avg_size:.0f}, max={chunk_stats['max_size']}")
            print(f"   ‚îî‚îÄ –ü–µ—Ä–≤—ã–µ 3 —Ä–∞–∑–º–µ—Ä–∞: {[len(c) for c in chunks[:3]]}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏
            if chunk_stats['with_context'] > 0:
                for i, chunk_text in enumerate(chunks[:2]):
                    if '\n' in chunk_text[:100]:  # –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        lines = chunk_text.split('\n', 2)
                        if len(lines) >= 2:
                            print(f"   üìå –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–∞ {i+1}: {lines[0][:80]}...")
                            break
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            if chunks:
                first_chunk_preview = chunks[0][:300].replace('\n', '\\n')
                print(f"   üìù –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫: {first_chunk_preview}...")
            
            return chunks
            
        except Exception as e:
            raise AppError(400, "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏")
    
   