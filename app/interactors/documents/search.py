
from typing import List, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer
from qdrant_client.http.models import FieldCondition, MatchValue, MatchAny

from app.dto.qdrant_filters import QdrantFilters
from app.repositories.documents import DocumentsRepository
from app.repositories.qdrant_embeddings import QdrantEmbeddingsRepository
from app.entities.documents import Document
from app.utils.collections import Collections


class SearchDocumentsInteractor:
    
    def __init__(
        self, 
        documents_repository: DocumentsRepository,
        qdrant_embeddings_repository: QdrantEmbeddingsRepository,
        sentence_transformer: SentenceTransformer
    ):
        self.documents_repository = documents_repository
        self.qdrant_embeddings_repository = qdrant_embeddings_repository
        self.sentence_transformer = sentence_transformer

    async def execute(
        self, 
        query: str, 
        limit: int = 10, 
        similarity_threshold: float = 0.7,
        document_id: str = None,
        document_types: List[str] = None
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ Qdrant
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            similarity_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
            document_id: ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            document_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        
        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "query: "
        # –í–ê–ñ–ù–û: E5 –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç "query: " –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ "passage: " –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        query_with_prefix = "query: " + query
        query_vector = self.sentence_transformer.encode(
            [query_with_prefix], 
            convert_to_numpy=True,
            normalize_embeddings=True
        )[0].tolist()
        
        print(f"üîç –ü–æ–∏—Å–∫: '{query}' (—Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º E5: 'query:')")

        # 2. –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        filter_conditions = []
        
        # –§–∏–ª—å—Ç—Ä –ø–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if document_id:
            filter_conditions.append(
                FieldCondition(
                    key="document_id",
                    match=MatchValue(value=document_id)
                )
            )
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if document_types:
            filter_conditions.append(
                FieldCondition(
                    key="document_type", 
                    match=MatchAny(any=document_types)
                )
            )
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å —É—Å–ª–æ–≤–∏—è
        filters = None
        if filter_conditions:
            filters = QdrantFilters(must=filter_conditions)
        
        search_results = await self.qdrant_embeddings_repository.search_similar(
            collection_name=Collections.DOCUMENT_EMBEDDINGS,
            query_vector=query_vector,
            limit=limit,
            similarity_threshold=similarity_threshold,
            filters=filters
        )

        # 3. –ø–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        matches: List[Dict[str, Any]] = []
        document_ids = list(set(result.payload.get("document_id") for result in search_results))
        
        if document_ids:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            documents = await self.documents_repository.get_all(
                where=[Document.id.in_(document_ids)]
            )
            documents_by_id = {str(doc.id): doc for doc in documents}
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            for result in search_results:
                doc_id = result.payload.get("document_id")
                doc = documents_by_id.get(doc_id)
                if doc:
                    chunk_content = result.payload.get("chunk_content", "")
                    
                    # –°–æ–∑–¥–∞–µ–º —É–º–Ω–æ–µ –ø—Ä–µ–≤—å—é —á–∞–Ω–∫–∞
                    preview = self._create_smart_preview(chunk_content, query)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    query_words = [word.lower().strip() for word in query.split() if len(word.strip()) > 2]
                    chunk_lower = chunk_content.lower()
                    text_matches = sum(1 for word in query_words if word in chunk_lower) if query_words else 0
                    
                    matches.append(
                        {
                            "document_id": doc_id,
                            "filename": doc.original_filename,
                            "content_type": doc.content_type,
                            "chunk": preview,
                            "full_chunk": chunk_content,
                            "similarity": round(result.score, 3),
                            "chunk_index": result.payload.get("chunk_index", 0),
                            "chunk_length": len(chunk_content),
                            "created_at": doc.created_at.isoformat() if hasattr(doc, 'created_at') else None,
                            "text_matches": text_matches,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤
                            "query_words": query_words,  # –°–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                            "has_text_match": text_matches > 0  # –ï—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                        }
                    )

        # 4. –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"DEBUG: Found {len(search_results)} raw results from Qdrant")
        print(f"DEBUG: Query words: {[word.lower().strip() for word in query.split() if len(word.strip()) > 2]}")
        print(f"DEBUG: Similarity range: {min(match['similarity'] for match in matches) if matches else 0:.3f} - {max(match['similarity'] for match in matches) if matches else 0:.3f}")
        
        # 5. –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        sorted_results = self._sort_results_optimally(matches, query)
        
        print(f"DEBUG: Returning {len(sorted_results)} filtered results")
        return sorted_results
    
    def _create_smart_preview(self, chunk_content: str, query: str, max_length: int = 300) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —É–º–Ω–æ–µ –ø—Ä–µ–≤—å—é —á–∞–Ω–∫–∞ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π
        
        Args:
            chunk_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–∞
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–≤—å—é
            
        Returns:
            –ü—Ä–µ–≤—å—é —á–∞–Ω–∫–∞ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π
        """
        if len(chunk_content) <= max_length:
            return chunk_content
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ —á–∞–Ω–∫–µ (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
        query_words = [word.lower() for word in query.split() if len(word) > 2]
        chunk_lower = chunk_content.lower()
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        word_positions = []
        for word in query_words:
            pos = chunk_lower.find(word)
            if pos != -1:
                word_positions.append(pos)
        
        if word_positions:
            # –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–≤—å—é —Å –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            start_pos = max(0, min(word_positions) - 50)
            end_pos = min(len(chunk_content), start_pos + max_length)
            
            preview = chunk_content[start_pos:end_pos]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∞–ª–∏
            if start_pos > 0:
                preview = "..." + preview
            if end_pos < len(chunk_content):
                preview = preview + "..."
                
            return preview
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞, –±–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ —á–∞–Ω–∫–∞
            return chunk_content[:max_length] + "..."
    
    def _sort_results_optimally(self, matches: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """
        –°–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –æ–±—Ä–∞–∑–æ–º:
        1. –ü–æ —É–±—ã–≤–∞–Ω–∏—é similarity (—Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        2. –ü–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é chunk_index (–ø–æ—Ä—è–¥–æ–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)
        3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            matches: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        if not matches:
            return matches
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        filtered_matches = self._filter_by_text_relevance(matches, query)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        documents_groups = {}
        for match in filtered_matches:
            doc_id = match["document_id"]
            if doc_id not in documents_groups:
                documents_groups[doc_id] = []
            documents_groups[doc_id].append(match)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ chunk_index
        for doc_id in documents_groups:
            documents_groups[doc_id].sort(key=lambda x: x["chunk_index"])
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        sorted_matches = []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π similarity —Å—Ä–µ–¥–∏ –∏—Ö —á–∞–Ω–∫–æ–≤
        doc_similarities = {}
        for doc_id, chunks in documents_groups.items():
            max_similarity = max(chunk["similarity"] for chunk in chunks)
            doc_similarities[doc_id] = max_similarity
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π similarity
        sorted_doc_ids = sorted(
            documents_groups.keys(), 
            key=lambda x: doc_similarities[x], 
            reverse=True
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –≤ –ø–æ—Ä—è–¥–∫–µ: –¥–æ–∫—É–º–µ–Ω—Ç -> chunk_index
        for doc_id in sorted_doc_ids:
            sorted_matches.extend(documents_groups[doc_id])
        
        return sorted_matches
    
    def _filter_by_text_relevance(self, matches: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            matches: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        if not query or not matches:
            return matches
        
        query_words = [word.lower().strip() for word in query.split() if len(word.strip()) > 2]
        if not query_words:
            return matches
        
        filtered_matches = []
        
        for match in matches:
            chunk_text = match.get("full_chunk", "").lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞–Ω–∫ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            contains_query_word = any(word in chunk_text for word in query_words)
            
            # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞, –¥–æ–±–∞–≤–ª—è–µ–º —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –≤–µ—Å–æ–º
            if contains_query_word:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤
                word_matches = sum(1 for word in query_words if word in chunk_text)
                match["text_relevance_score"] = word_matches / len(query_words)
                filtered_matches.append(match)
            else:
                # –ï—Å–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç, –Ω–æ similarity –≤—ã—Å–æ–∫–∞—è, –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º
                # –Ω–æ —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
                if match["similarity"] > 0.8:  # –í—ã—Å–æ–∫–∞—è similarity
                    match["text_relevance_score"] = 0.1
                    filtered_matches.append(match)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏, –∑–∞—Ç–µ–º –ø–æ similarity
        filtered_matches.sort(
            key=lambda x: (x.get("text_relevance_score", 0), x["similarity"]), 
            reverse=True
        )
        
        return filtered_matches