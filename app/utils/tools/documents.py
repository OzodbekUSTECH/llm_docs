"""
Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
"""
from typing import List, Dict, Any, Optional
from app.repositories.documents import DocumentsRepository
from app.repositories.qdrant_embeddings import QdrantEmbeddingsRepository
from app.entities.documents import Document
from app.di.containers import app_container
from sentence_transformers import SentenceTransformer
from app.dto.ai_models import TextContent
from app.dto.pagination import InfiniteScrollRequest


async def search_documents(query: str, limit: int = 10) -> List[TextContent]:
    """
    Search for relevant documents using semantic vector search. Returns formatted text with document information.
    
    Use this tool to:
    - Find documents matching a query (e.g., company names, owners, directors, contracts, specifications, etc.)
    - Get relevant excerpts from documents with preview
    - Obtain document IDs for full content retrieval if needed

    Arguments:
        query (str): Natural language search query. Be specific and descriptive. Examples:
            - "Floriana Impex owner and directors information"
            - "vessel technical specifications and capabilities"
            - "contract terms, conditions and payment details"
            - "company registration and legal entity information"
        limit (int, optional): Maximum number of documents to return. Default is 10.

    Returns:
        List[TextContent]: Formatted text containing:
            - ğŸ“„ Found X documents matching 'query'
            - For each document:
                - **N. filename** - Document name
                - ğŸ†” ID: document_id | ğŸ“Š Size: file_size bytes
                - ğŸ“… Uploaded: upload_date
                - ğŸ‘ï¸ Preview: text preview with key fragments
                - â­ Relevance: score (0.0-1.0)
    
    If no results found, returns "No documents found matching query: 'query'".
    """
    try:
        print(f"[SEARCH] Query: '{query}', limit: {limit}")
        
        async with app_container() as container:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
            qdrant_embeddings_repository: QdrantEmbeddingsRepository = await container.get(QdrantEmbeddingsRepository)
            sentence_transformer: SentenceTransformer = await container.get(SentenceTransformer)
            
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ¼ E5
            # Ğ’ĞĞ–ĞĞ: E5 Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ "query: " Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
            query_with_prefix = "query: " + query
            query_vector = sentence_transformer.encode(
                [query_with_prefix], 
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False,
            )[0].tolist()
            
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
            search_results = await qdrant_embeddings_repository.search_similar(
                query_vector=query_vector,
                limit=limit * 3,  # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸
                similarity_threshold=0.4,
            )
            
            print(f"[SEARCH] Found {len(search_results)} raw chunks")
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"No documents found matching query: '{query}'"
                )]
            
            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼
            documents_dict = {}
            for result in search_results:
                doc_id = result["document_id"]
                if doc_id not in documents_dict:
                    documents_dict[doc_id] = {
                        "document_id": doc_id,
                        "chunks": [],
                        "max_similarity": 0,
                        "filename": result.get("filename", ""),
                        "content_type": result.get("content_type", ""),
                    }
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ‡Ğ°Ğ½ĞºĞ°
                chunk_content = result["chunk_content"]
                documents_dict[doc_id]["chunks"].append({
                    "content": chunk_content,
                    "similarity": result["similarity"],
                    "chunk_index": result["chunk_index"]
                })
                
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ
                if result["similarity"] > documents_dict[doc_id]["max_similarity"]:
                    documents_dict[doc_id]["max_similarity"] = result["similarity"]
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ´Ğ»Ñ LLM
            results = []
            for doc_id, doc_data in documents_dict.items():
                # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑÑƒ
                doc_data["chunks"].sort(
                    key=lambda x: (-x["similarity"], x["chunk_index"])
                )
                
                # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¢ĞĞŸ-3 Ñ‡Ğ°Ğ½ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ²ÑŒÑ
                best_chunks = doc_data["chunks"][:3]
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ²ÑŒÑ Ğ¸Ğ· Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
                preview_parts = []
                for chunk in best_chunks:
                    content = chunk["content"]
                    if len(content) > 200:  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ²ÑŒÑ
                        content = content[:200] + "..."
                    preview_parts.append(content)
                
                preview_text = " | ".join(preview_parts)
                
                # ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ñ„Ğ°Ğ¹Ğ»Ğ°
                total_chunk_length = sum(len(chunk["content"]) for chunk in doc_data["chunks"])
                
                results.append({
                    "filename": doc_data["filename"],  # ğŸ“„ ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°
                    "id": doc_id,  # ğŸ†” ID Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
                    "file_size": total_chunk_length,  # ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ² Ğ±Ğ°Ğ¹Ñ‚Ğ°Ñ…
                    "uploaded_at": "N/A",  # ğŸ“… Ğ”Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ (Ğ½Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ¸Ğ· Qdrant)
                    "preview": preview_text,  # ğŸ‘ï¸ ĞŸÑ€ĞµĞ²ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ°
                    "relevance_score": round(doc_data["max_similarity"], 3),  # â­ Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ
                    "content_type": doc_data["content_type"],  # Ğ¢Ğ¸Ğ¿ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
                    "chunks_count": len(doc_data["chunks"]),  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
                })
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
            results.sort(key=lambda x: x["relevance_score"], reverse=True)
            
          
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ´Ğ»Ñ LLM
            response = f"ğŸ“„ Found {len(results)} documents matching '{query}':\n\n"
            
            for i, doc in enumerate(results, 1):
                response += f"**{i}. {doc['filename']}**\n"
                response += f"   ğŸ†” ID: {doc['id']} | ğŸ“Š Size: {doc['file_size']:,} bytes\n"
                response += f"   ğŸ“… Uploaded: {doc['uploaded_at']}\n"
                response += f"   ğŸ‘ï¸ Preview: {doc['preview']}\n"
                response += f"   â­ Relevance: {doc['relevance_score']:.3f}\n\n"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        print(f"[SEARCH ERROR] {str(e)}")
        return [TextContent(
            type="text",
            text=f"Error searching documents: {str(e)}"
        )]


async def get_document_by_id(document_id: str, include_content: bool = False) -> List[TextContent]:
    """
    Get document information by document ID.
    
    Use this tool when you need:
    - Document metadata and basic information
    - Truncated content preview (if include_content=True)
    - Document status and properties
    
    Args:
        document_id: The unique ID of the document (UUID string from search results)
        include_content: If True, includes truncated content preview (max 2000 chars)
        
    Returns:
        List[TextContent]: Formatted document information
    """
    try:
        async with app_container() as container:
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
            document = await documents_repository.get_one(
                where=[Document.id == document_id]
            )
            
            if not document:
                return [TextContent(
                    type="text",
                    text=f"âŒ Document with ID {document_id} not found"
                )]
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            content_length = len(document.content) if document.content else 0
            status = document.status.value if hasattr(document.status, 'value') else str(document.status)
            created_at = document.created_at.isoformat() if hasattr(document, 'created_at') else "N/A"
            
            response = f"ğŸ“„ **Document Information**\n\n"
            response += f"**Filename:** {document.original_filename}\n"
            response += f"**ID:** {document_id}\n"
            response += f"**Content Type:** {document.content_type}\n"
            response += f"**Status:** {status}\n"
            response += f"**Content Length:** {content_length:,} characters\n"
            response += f"**File Hash:** {document.file_hash}\n"
            response += f"**Created At:** {created_at}\n"
            
            if include_content and document.content:
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
                truncated_content = document.content
                if len(truncated_content) > 2000:
                    truncated_content = truncated_content[:2000] + "..."
                
                response += f"\n**Content Preview:**\n"
                response += f"```\n{truncated_content}\n```\n"
                response += f"\n*Note: Content is truncated to 2000 characters. Use get_document_full_content for complete content.*"
            else:
                response += f"\n*Use get_document_full_content to retrieve the complete document content.*"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error retrieving document: {str(e)}"
        )]


async def get_document_full_content(document_id: str, chunk_size: int = 3000, chunk_index: int = 0) -> List[TextContent]:
    """
    Get full document content in chunks for LLM processing.
    
    Use this tool when you need:
    - Complete document content (not truncated)
    - Large documents that need to be processed in parts
    - Full text analysis and processing
    
    Args:
        document_id: The unique ID of the document (UUID string from search results)
        chunk_size: Size of each content chunk in characters (default: 3000)
        chunk_index: Which chunk to retrieve (0-based, default: 0)
        
    Returns:
        List[TextContent]: Document content chunk with pagination info
    """
    try:
        async with app_container() as container:
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
            document = await documents_repository.get_one(
                where=[Document.id == document_id]
            )
            
            if not document:
                return [TextContent(
                    type="text",
                    text=f"âŒ Document with ID {document_id} not found"
                )]
            
            if not document.content:
                return [TextContent(
                    type="text",
                    text=f"âŒ Document '{document.original_filename}' has no content"
                )]
            
            # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
            content = document.content
            total_chunks = (len(content) + chunk_size - 1) // chunk_size  # ĞĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ²ĞµÑ€Ñ…
            
            if chunk_index >= total_chunks:
                return [TextContent(
                    type="text",
                    text=f"âŒ Chunk index {chunk_index} is out of range. Document has {total_chunks} chunks (0-{total_chunks-1})"
                )]
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº
            start_pos = chunk_index * chunk_size
            end_pos = min(start_pos + chunk_size, len(content))
            chunk_content = content[start_pos:end_pos]
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            response = f"ğŸ“„ **Full Document Content - Chunk {chunk_index + 1}/{total_chunks}**\n\n"
            response += f"**Document:** {document.original_filename}\n"
            response += f"**ID:** {document_id}\n"
            response += f"**Chunk Size:** {chunk_size} characters\n"
            response += f"**Position:** {start_pos:,}-{end_pos:,} of {len(content):,} characters\n\n"
            response += f"**Content:**\n```\n{chunk_content}\n```\n\n"
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸
            if total_chunks > 1:
                response += f"**Navigation:**\n"
                if chunk_index > 0:
                    response += f"- Previous chunk: chunk_index={chunk_index-1}\n"
                if chunk_index < total_chunks - 1:
                    response += f"- Next chunk: chunk_index={chunk_index+1}\n"
                response += f"- All chunks: 0 to {total_chunks-1}\n"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error retrieving document content: {str(e)}"
        )]


async def query_documents(
    filters: Optional[Dict[str, Any]] = None,
    order_by: Optional[str] = None,
    limit: Optional[int] = None,
    count_only: bool = False,
    group_by: Optional[str] = None
) -> List[TextContent]:
    """
    Query documents with flexible filtering, sorting, and aggregation.
    
    Use this tool when you need:
    - Count documents (e.g., "how many documents do I have?")
    - Filter by status, content type, date range, etc.
    - Group documents by specific fields
    - Sort documents by various criteria
    - Get statistics about your document collection
    
    Args:
        filters: Dictionary of filters to apply. Available fields:
            - status: DocumentStatus (PENDING, PROCESSING, COMPLETED, FAILED)
            - content_type: str (e.g., "application/pdf", "text/plain")
            - filename: str (partial match)
            - original_filename: str (partial match)
            - created_after: str (ISO date, e.g., "2024-01-01")
            - created_before: str (ISO date, e.g., "2024-12-31")
            - min_content_length: int (minimum content length)
            - max_content_length: int (maximum content length)
        order_by: Field to sort by (e.g., "created_at", "filename", "content_length")
        limit: Maximum number of documents to return
        count_only: If True, returns only count without document details
        group_by: Field to group by (e.g., "status", "content_type")
        
    Returns:
        List[TextContent]: Query results with statistics and document information
    """
    try:
        async with app_container() as container:
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ WHERE
            where_conditions = []
            
            if filters:
                if "status" in filters:
                    where_conditions.append(Document.status == filters["status"])
                
                if "content_type" in filters:
                    where_conditions.append(Document.content_type == filters["content_type"])
                
                if "filename" in filters:
                    where_conditions.append(Document.filename.ilike(f"%{filters['filename']}%"))
                
                if "original_filename" in filters:
                    where_conditions.append(Document.original_filename.ilike(f"%{filters['original_filename']}%"))
                
                if "created_after" in filters:
                    from datetime import datetime
                    created_after = datetime.fromisoformat(filters["created_after"].replace('Z', '+00:00'))
                    where_conditions.append(Document.created_at >= created_after)
                
                if "created_before" in filters:
                    from datetime import datetime
                    created_before = datetime.fromisoformat(filters["created_before"].replace('Z', '+00:00'))
                    where_conditions.append(Document.created_at <= created_before)
                
                if "min_content_length" in filters:
                    where_conditions.append(Document.content.length() >= filters["min_content_length"])
                
                if "max_content_length" in filters:
                    where_conditions.append(Document.content.length() <= filters["max_content_length"])
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            request_query = None
            if limit:
                request_query = InfiniteScrollRequest(limit=limit, offset=0)
            
            documents = await documents_repository.get_all(
                request_query=request_query,
                where=where_conditions if where_conditions else None
            )
            
            # Ğ•ÑĞ»Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸Ğº
            if count_only:
                total_count = len(documents)
                response = f"ğŸ“Š **Document Count**\n\n"
                response += f"**Total documents:** {total_count}\n"
                
                if filters:
                    response += f"\n**Applied filters:**\n"
                    for key, value in filters.items():
                        response += f"- {key}: {value}\n"
                
                return [TextContent(type="text", text=response)]
            
            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°
            if group_by:
                groups = {}
                for doc in documents:
                    if group_by == "status":
                        key = doc.status.value if hasattr(doc.status, 'value') else str(doc.status)
                    elif group_by == "content_type":
                        key = doc.content_type
                    elif group_by == "filename":
                        key = doc.original_filename.split('.')[-1] if '.' in doc.original_filename else "unknown"
                    else:
                        key = "unknown"
                    
                    if key not in groups:
                        groups[key] = []
                    groups[key].append(doc)
                
                response = f"ğŸ“Š **Documents grouped by {group_by}**\n\n"
                for group_name, group_docs in groups.items():
                    response += f"**{group_name}:** {len(group_docs)} documents\n"
                    for doc in group_docs[:3]:  # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ
                        response += f"  - {doc.original_filename} ({doc.status.value if hasattr(doc.status, 'value') else doc.status})\n"
                    if len(group_docs) > 3:
                        response += f"  ... and {len(group_docs) - 3} more\n"
                    response += "\n"
                
                return [TextContent(type="text", text=response)]
            
            # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            if not documents:
                response = f"ğŸ“„ **No documents found**\n\n"
                if filters:
                    response += f"**Applied filters:**\n"
                    for key, value in filters.items():
                        response += f"- {key}: {value}\n"
                return [TextContent(type="text", text=response)]
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            if order_by:
                if order_by == "created_at":
                    documents.sort(key=lambda x: x.created_at, reverse=True)
                elif order_by == "filename":
                    documents.sort(key=lambda x: x.original_filename.lower())
                elif order_by == "content_length":
                    documents.sort(key=lambda x: len(x.content) if x.content else 0, reverse=True)
                elif order_by == "status":
                    documents.sort(key=lambda x: x.status.value if hasattr(x.status, 'value') else str(x.status))
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            response = f"ğŸ“„ **Found {len(documents)} documents**\n\n"
            
            if filters:
                response += f"**Applied filters:**\n"
                for key, value in filters.items():
                    response += f"- {key}: {value}\n"
                response += "\n"
            
            if order_by:
                response += f"**Sorted by:** {order_by}\n\n"
            
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
            for i, doc in enumerate(documents, 1):
                content_length = len(doc.content) if doc.content else 0
                status = doc.status.value if hasattr(doc.status, 'value') else str(doc.status)
                created_at = doc.created_at.isoformat() if hasattr(doc, 'created_at') else "N/A"
                
                response += f"**{i}. {doc.original_filename}**\n"
                response += f"   ğŸ†” ID: {doc.id}\n"
                response += f"   ğŸ“Š Status: {status}\n"
                response += f"   ğŸ“„ Type: {doc.content_type}\n"
                response += f"   ğŸ“ Size: {content_length:,} characters\n"
                response += f"   ğŸ“… Created: {created_at}\n\n"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error querying documents: {str(e)}"
        )]


