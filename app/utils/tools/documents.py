"""
Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸
"""
from typing import List, Dict, Any, Optional
from sqlalchemy import and_, func
from app.repositories.documents import DocumentsRepository
from app.repositories.qdrant_embeddings import QdrantEmbeddingsRepository
from app.entities.documents import Document
from app.di.containers import app_container
from sentence_transformers import SentenceTransformer
from app.dto.ai_models import TextContent
from app.dto.pagination import InfiniteScrollRequest
from app.utils.collections import Collections
from qdrant_client.http.models import FieldCondition, MatchAny
from app.dto.qdrant_filters import QdrantFilters
from app.utils.enums import DocumentType

def format_datetime(date_obj, include_time=True):
    """Format datetime object to readable string"""
    if not date_obj:
        return "Unknown"
    
    try:
        if include_time:
            return date_obj.strftime("%B %d, %Y at %H:%M")
        else:
            return date_obj.strftime("%B %d, %Y")
    except Exception:
        return "Invalid Date"

async def search_documents(query: str, limit: int = 10, document_ids: Optional[List[str]] = None, document_types: Optional[List[str]] = None) -> List[TextContent]:
    """
    Search for relevant documents using semantic vector search. Returns document metadata and keywords.
    
    Use this tool to:
    - Find documents matching a query (e.g., company names, owners, directors, contracts, specifications, etc.)
    - Get document metadata and extracted keywords
    - Search within specific documents by providing document IDs
    - Obtain document IDs for further analysis if needed
    - Documents are automatically categorized by type (INVOICE, CONTRACT, COO, COA, COW, BL, LC, FINANCIAL, OTHER)

    Arguments:
        query (str): Natural language search query. Be specific and descriptive. Examples:
            - "Floriana Impex owner and directors information"
            - "vessel technical specifications and capabilities"
            - "contract terms, conditions and payment details"
            - "company registration and legal entity information"
            - "invoice payment terms" - will find INVOICE type documents
            - "certificate of origin" - will find COO type documents
        
        Usage with document_types parameter:
            - search_documents("payment terms", document_types=["INVOICE"]) - only invoices
            - search_documents("weight specifications", document_types=["COW", "COA"]) - certificates only
            - search_documents("contract details", document_types=["CONTRACT", "BL"]) - contracts and bills of lading
        limit (int, optional): Maximum number of documents to return. Default is 10.
        document_ids (List[str], optional): List of specific document IDs to search within. 
            If provided, search will be limited to these documents only. Useful for large document collections.
        document_types (List[str], optional): List of document types to filter by. Examples:
            - ["INVOICE"] - only invoices
            - ["CONTRACT", "INVOICE"] - only contracts and invoices
            - ["COO", "COA", "COW"] - only certificates

    Returns:
        List[TextContent]: Formatted text containing:
            - ğŸ“„ Found X documents matching 'query'
            - For each document:
                - **N. filename** - Document name
                - ğŸ†” ID: document_id | ğŸ“Š Size: file_size bytes
                - ğŸ“‹ Type: document_type | ğŸ“„ Format: content_type
                - ğŸ”‘ Keywords: extracted key information
                - â­ Relevance: score (0.0-1.0)
    
    Document Types:
        - INVOICE: Invoices and bills
        - CONTRACT: Contracts and agreements
        - COO: Certificate of Origin
        - COA: Certificate of Analysis
        - COW: Certificate of Weight
        - BL: Bill of Lading
        - FINANCIAL: Financial reports and statements
        - OTHER: Other document types
    
    If no results found, returns "No documents found matching query: 'query'".
    """
    try:
        print(f"[SEARCH] Query: '{query}', limit: {limit}, document_ids: {document_ids}, document_types: {document_types}")
        
        async with app_container() as container:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
            qdrant_embeddings_repository: QdrantEmbeddingsRepository = await container.get(QdrantEmbeddingsRepository)
            sentence_transformer: SentenceTransformer = await container.get(SentenceTransformer)
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ¼ E5
            # Ğ’ĞĞ–ĞĞ: E5 Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ "query: " Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
            query_with_prefix = "query: " + query
            query_vector = sentence_transformer.encode(
                [query_with_prefix], 
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False,
            )[0].tolist()
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
            filter_conditions = []
            
            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ ID Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            if document_ids:
                filter_conditions.append(
                    FieldCondition(
                        key="document_id",
                        match=MatchAny(any=document_ids)
                    )
                )
            
            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            if document_types:
                filter_conditions.append(
                    FieldCondition(
                        key="document_type",
                        match=MatchAny(any=document_types)
                    )
                )
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ
            filters = None
            if filter_conditions:
                filters = QdrantFilters(must=filter_conditions)
            
            search_results = await qdrant_embeddings_repository.search_similar(
                collection_name=Collections.DOCUMENT_EMBEDDINGS,
                query_vector=query_vector,
                limit=limit,
                similarity_threshold=0.7,  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
                filters=filters
            )
            
            print(f"[SEARCH] Found {len(search_results)} raw chunks")
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"No documents found matching query: '{query}'"
                )]
            
            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ
            documents_scores = {}
            for result in search_results:
                doc_id = result.payload.get("document_id")
                if doc_id not in documents_scores:
                    documents_scores[doc_id] = {
                        "max_similarity": 0,
                        "filename": result.payload.get("filename", ""),
                        "content_type": result.payload.get("content_type", ""),
                        "document_type": result.payload.get("document_type", "OTHER"),
                        "chunks": []  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
                    }
                
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‡Ğ°Ğ½Ğº
                if result.score > documents_scores[doc_id]["max_similarity"]:
                    documents_scores[doc_id]["max_similarity"] = result.score
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‡Ğ°Ğ½Ğº Ñ ĞµĞ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ
                documents_scores[doc_id]["chunks"].append({
                    "content": result.payload.get("chunk_content", ""),
                    "score": result.score,
                    "chunk_index": result.payload.get("chunk_index", 0)
                })
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
            for doc_id, doc_data in documents_scores.items():
                doc_data["chunks"].sort(key=lambda x: x["score"], reverse=True)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ‘Ğ” Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼
            doc_ids = list(documents_scores.keys())
            
            documents = await documents_repository.get_all(
                where=[Document.id.in_(doc_ids)]
            )
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼Ğ°Ğ¿Ñƒ id -> document Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°
            documents_map = {str(doc.id): doc for doc in documents}
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ´Ğ»Ñ LLM
            results = []
            for doc_id, doc_data in documents_scores.items():
                doc = documents_map.get(doc_id)
                if not doc:
                    continue
                
                
                keywords_text = " | ".join(doc.keywords) if doc.keywords else "No keywords"
                
                # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ²ÑĞµÑ… Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğ¹
                chunks_content = []
                for i, chunk in enumerate(doc_data["chunks"]):
                    chunks_content.append(f"Chunk {i+1} (score: {chunk['score']:.3f}): {chunk['content']}")
                
                results.append({
                    "filename": doc.original_filename,
                    "id": doc_id,
                    "file_size": len(doc.content) if doc.content else 0,
                    "keywords": keywords_text,
                    "relevance_score": round(doc_data["max_similarity"], 3),
                    "content_type": doc.content_type,
                    "document_type": doc.type.value,
                    "chunks": chunks_content,
                    "total_chunks": len(doc_data["chunks"])
                })
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
            results.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ´Ğ»Ñ LLM
            filter_info = []
            if document_ids:
                filter_info.append(f"limited to {len(document_ids)} specified documents")
            if document_types:
                filter_info.append(f"filtered by types: {', '.join(document_types)}")
            
            if filter_info:
                response = f"ğŸ“„ Found {len(results)} documents matching '{query}' ({' and '.join(filter_info)}):\n\n"
            else:
                response = f"ğŸ“„ Found {len(results)} documents matching '{query}':\n\n"
            
            for i, doc in enumerate(results, 1):
                response += f"**{i}. {doc['filename']}**\n"
                response += f"   ğŸ†” ID: {doc['id']} | ğŸ“Š Size: {doc['file_size']:,} characters\n"
                response += f"   ğŸ“‹ Type: {doc['document_type']} | ğŸ“„ Format: {doc['content_type']}\n"
                if doc['keywords']:
                    response += f"   ğŸ”‘ Keywords: {doc['keywords']}\n"
                response += f"   â­ Relevance: {doc['relevance_score']:.3f}\n"
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ÑĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸
                if doc['chunks']:
                    response += f"   ğŸ“ Found {doc['total_chunks']} relevant chunks:\n"
                    for chunk in doc['chunks']:
                        response += f"      â€¢ {chunk}\n"
                
                response += "\n"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        print(f"[SEARCH ERROR] {str(e)}")
        return [TextContent(
            type="text",
            text=f"Error searching documents: {str(e)}"
        )]


async def get_document_by_id(document_id: str) -> List[TextContent]:
    """
    Get document information by document ID including metadata and keywords.
    
    Use this tool when you need:
    - Document metadata and basic information
    - Extracted keywords and their values
    - Document status and properties
    
    Args:
        document_id: The unique ID of the document (UUID string from search results)
        
    Returns:
        List[TextContent]: Formatted document information with keywords
    """
    try:
        async with app_container() as container:
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
            document = await documents_repository.get_one(
                where=[Document.id == document_id]
            )
            
            if not document:
                return [TextContent(
                    type="text",
                    text=f"âŒ Document with ID {document_id} not found"
                )]
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            content_length = len(document.content) if document.content else 0
            status = document.status.value if hasattr(document.status, 'value') else str(document.status)
            created_at = document.created_at.isoformat() if hasattr(document, 'created_at') else "N/A"
            
            response = f"ğŸ“„ **Document Information**\n\n"
            response += f"**Filename:** {document.original_filename}\n"
            response += f"**ID:** {document_id}\n"
            response += f"**Content Type:** {document.content_type}\n"
            response += f"**Document Type:** {document.type.value}\n"
            response += f"**Status:** {status}\n"
            response += f"**Content Length:** {content_length:,} characters\n"
            response += f"**File Hash:** {document.file_hash}\n"
            response += f"**Created At:** {created_at}\n"
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if document.keywords:
                response += f"\n**ğŸ”‘ Extracted Keywords:**\n"
                for key, value in document.keywords.items():
                    if value:
                        response += f"- **{key}**: {value}\n"
            else:
                response += f"\n**ğŸ”‘ Keywords:** No keywords extracted\n"
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error retrieving document: {str(e)}"
        )]




async def query_documents(
    filters: Optional[Dict[str, Any]] = None,
    order_by: Optional[str] = None,
    limit: Optional[int] = None,
    count_only: bool = False,
    group_by: Optional[str] = None
) -> List[TextContent]:
    """
    Query documents with flexible filtering, sorting, and aggregation.
    
    Use this tool when you need:
    - Count documents (e.g., "how many documents do I have?")
    - Filter by status, content type, document type, date range, etc.
    - Group documents by specific fields
    - Sort documents by various criteria
    - Get statistics about your document collection
    
    Args:
        filters: Dictionary of filters to apply. Available fields:
            - status: DocumentStatus (PENDING, PROCESSING, COMPLETED, FAILED)
            - content_type: str (e.g., "application/pdf", "text/plain")
            - filename: str (partial match)
            - document_type: DocumentType (e.g., INVOICE, CONTRACT, COO, COA, COW, BL, LC, FINANCIAL, OTHER)
            - original_filename: str (partial match)
            - created_after: str (ISO date, e.g., "2024-01-01")
            - created_before: str (ISO date, e.g., "2024-12-31")
            - min_content_length: int (minimum content length)
            - max_content_length: int (maximum content length)
        order_by: Field to sort by (e.g., "created_at", "filename", "content_length")
        limit: Maximum number of documents to return
        count_only: If True, returns only count without document details
        group_by: Field to group by (e.g., "status", "content_type")
        
    Returns:
        List[TextContent]: Query results with statistics and document information
    """
    try:
        async with app_container() as container:
            documents_repository: DocumentsRepository = await container.get(DocumentsRepository)
            
            # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ WHERE
            where_conditions = []
            
            if filters:
                if "status" in filters:
                    where_conditions.append(Document.status == filters["status"])
                
                if "content_type" in filters:
                    where_conditions.append(Document.content_type == filters["content_type"])
                    
                if "document_type" in filters:
                    where_conditions.append(Document.type == DocumentType(filters["document_type"]))
                if "filename" in filters:
                    where_conditions.append(Document.filename.ilike(f"%{filters['filename']}%"))
                
                if "original_filename" in filters:
                    where_conditions.append(Document.original_filename.ilike(f"%{filters['original_filename']}%"))
                
                if "created_after" in filters:
                    from datetime import datetime
                    created_after = datetime.fromisoformat(filters["created_after"].replace('Z', '+00:00'))
                    where_conditions.append(Document.created_at >= created_after)
                
                if "created_before" in filters:
                    from datetime import datetime
                    created_before = datetime.fromisoformat(filters["created_before"].replace('Z', '+00:00'))
                    where_conditions.append(Document.created_at <= created_before)
                
                if "min_content_length" in filters:
                    where_conditions.append(Document.content.length() >= filters["min_content_length"])
                
                if "max_content_length" in filters:
                    where_conditions.append(Document.content.length() <= filters["max_content_length"])
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            request_query = None
            if limit:
                request_query = InfiniteScrollRequest(limit=limit, offset=0)
            
            documents = await documents_repository.get_all(
                request_query=request_query,
                where=where_conditions if where_conditions else None
            )
            
            # Ğ•ÑĞ»Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‡ĞµÑ‚Ñ‡Ğ¸Ğº
            if count_only:
                total_count = len(documents)
                response = f"ğŸ“Š **Document Count**\n\n"
                response += f"**Total documents:** {total_count}\n"
                
                if filters:
                    response += f"\n**Applied filters:**\n"
                    for key, value in filters.items():
                        response += f"- {key}: {value}\n"
                
                return [TextContent(type="text", text=response)]
            
            # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°
            if group_by:
                groups = {}
                for doc in documents:
                    if group_by == "status":
                        key = doc.status.value if hasattr(doc.status, 'value') else str(doc.status)
                    elif group_by == "content_type":
                        key = doc.content_type
                    elif group_by == "filename":
                        key = doc.original_filename.split('.')[-1] if '.' in doc.original_filename else "unknown"
                    else:
                        key = "unknown"
                    
                    if key not in groups:
                        groups[key] = []
                    groups[key].append(doc)
                
                response = f"ğŸ“Š **Documents grouped by {group_by}**\n\n"
                for group_name, group_docs in groups.items():
                    response += f"**{group_name}:** {len(group_docs)} documents\n"
                    for doc in group_docs[:3]:  # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ
                        response += f"  - {doc.original_filename} ({doc.status.value if hasattr(doc.status, 'value') else doc.status})\n"
                    if len(group_docs) > 3:
                        response += f"  ... and {len(group_docs) - 3} more\n"
                    response += "\n"
                
                return [TextContent(type="text", text=response)]
            
            # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
            if not documents:
                response = f"ğŸ“„ **No documents found**\n\n"
                if filters:
                    response += f"**Applied filters:**\n"
                    for key, value in filters.items():
                        response += f"- {key}: {value}\n"
                return [TextContent(type="text", text=response)]
            
            # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
            if order_by:
                if order_by == "created_at":
                    documents.sort(key=lambda x: x.created_at, reverse=True)
                elif order_by == "filename":
                    documents.sort(key=lambda x: x.original_filename.lower())
                elif order_by == "content_length":
                    documents.sort(key=lambda x: len(x.content) if x.content else 0, reverse=True)
                elif order_by == "status":
                    documents.sort(key=lambda x: x.status.value if hasattr(x.status, 'value') else str(x.status))
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
            response = f"ğŸ“„ **Found {len(documents)} documents**\n\n"
            
            if filters:
                response += f"**Applied filters:**\n"
                for key, value in filters.items():
                    response += f"- {key}: {value}\n"
                response += "\n"
            
            if order_by:
                response += f"**Sorted by:** {order_by}\n\n"
            
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
            for i, doc in enumerate(documents, 1):
                content_length = len(doc.content) if doc.content else 0
                status = doc.status.value if hasattr(doc.status, 'value') else str(doc.status)
                created_at = doc.created_at.isoformat() if hasattr(doc, 'created_at') else "N/A"
                
                response += f"**{i}. {doc.original_filename}**\n"
                response += f"   ğŸ†” ID: {doc.id}\n"
                response += f"   ğŸ“Š Status: {status}\n"
                response += f"   ğŸ“„ Content Type: {doc.content_type}\n"
                response += f"   ğŸ“„ Document Type: {doc.type}\n"
                response += f"   ğŸ“ Size: {content_length:,} characters\n"
                response += f"   ğŸ“… Created: {created_at}\n\n"
            
            return [TextContent(type="text", text=response)]
            
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error querying documents: {str(e)}"
        )]


async def search_documents_by_keywords(
    keyword: str, 
    value: Optional[str] = None, 
    document_types: Optional[List[str]] = None, 
    limit: int = 10
) -> List[TextContent]:
    """
    Search for documents by specific keywords extracted from their content.
    
    This tool allows you to find documents containing specific information like vessel names,
    invoice numbers, contract details, etc. Use this when you need to find documents with
    specific data points or values.
    
    Arguments:
        keyword (str): The specific keyword to search for (e.g., 'vessel', 'invoice_number', 'contract_number', 'seller', 'buyer')
        value (Optional[str]): The value to search for within that keyword field. If not provided, finds all documents with this keyword.
        document_types (Optional[List[str]]): List of document types to search within. If not provided, searches all types.
        limit (int): Maximum number of documents to return. Default is 10.
    
    Returns:
        List[TextContent]: Formatted results with document information and matching keyword values.
    """
    try:
        # Get repositories from DI container
        async with app_container() as container:
            documents_repository = await container.get(DocumentsRepository)
            
            # Build query conditions for database search
            conditions = []
            
            # Add document type filter if specified
            if document_types:
                conditions.append(Document.type.in_(document_types))
            
            # Add keyword search condition using JSON operations
            if value:
                # Search for documents where the keyword exists and contains the value
                keyword_condition = and_(
                    func.jsonb_exists(Document.keywords, keyword),
                    func.jsonb_path_exists(
                        Document.keywords,
                        f'$."{keyword}" ? (@ like_regex "{value}" flag "i")'
                    )
                )
            else:
                # Search for documents where the keyword exists (any value)
                keyword_condition = func.jsonb_exists(Document.keywords, keyword)
            
            conditions.append(keyword_condition)
            
            # Get documents matching the conditions with limit
            from app.dto.pagination import InfiniteScrollRequest
            request = InfiniteScrollRequest(limit=limit, offset=0)
            documents = await documents_repository.get_all(
                request_query=request,
                where=conditions,
            )
            
            # Prepare matching documents with keyword values
            matching_documents = []
            for doc in documents:
                if not doc.keywords or keyword not in doc.keywords:
                    continue
                
                keyword_data = doc.keywords[keyword]
                if isinstance(keyword_data, dict):
                    keyword_value = keyword_data.get('value', '')
                else:
                    keyword_value = str(keyword_data)
                
                matching_documents.append((doc, keyword_value))
            
            if not matching_documents:
                if value:
                    return [TextContent(
                        type="text",
                        text=f"ğŸ” No documents found with keyword '{keyword}' containing value '{value}'"
                    )]
                else:
                    return [TextContent(
                        type="text",
                        text=f"ğŸ” No documents found with keyword '{keyword}'"
                    )]
            
            # Format results
            if value:
                response = f"ğŸ” **Found {len(matching_documents)} documents with keyword '{keyword}' containing '{value}'**\n\n"
            else:
                response = f"ğŸ” **Found {len(matching_documents)} documents with keyword '{keyword}'**\n\n"
            
            for i, (doc, keyword_value) in enumerate(matching_documents, 1):
                # Format creation date
                created_at = format_datetime(doc.created_at, include_time=True)
                
                # Get document type icon and name
                type_icons = {
                    DocumentType.INVOICE: "ğŸ“„",
                    DocumentType.CONTRACT: "ğŸ“‹", 
                    DocumentType.COO: "ğŸŒ",
                    DocumentType.COA: "ğŸ§ª",
                    DocumentType.COW: "âš–ï¸",
                    DocumentType.COQ: "ğŸ†",
                    DocumentType.BL: "ğŸš¢",
                    DocumentType.LC: "ğŸ’³",
                    DocumentType.FINANCIAL: "ğŸ’°",
                    DocumentType.OTHER: "ğŸ“"
                }
                type_icon = type_icons.get(doc.type, "ğŸ“„")
                
                response += f"**{i}. {type_icon} {doc.original_filename}**\n"
                response += f"   ğŸ†” ID: `{doc.id}`\n"
                response += f"   ğŸ“‹ Type: {doc.type.value}\n"
                response += f"   ğŸ”‘ **{keyword}**: {keyword_value}\n"
                response += f"   ğŸ“… Created: {created_at}\n"
                
                response += "\n"
            
            return [TextContent(type="text", text=response)]
        
    except Exception as e:
        return [TextContent(
            type="text",
            text=f"âŒ Error searching documents by keywords: {str(e)}"
        )]


