import base64
from dishka import Provider, Scope, provide, provide_all

from sentence_transformers import SentenceTransformer
from qdrant_client import AsyncQdrantClient
from transformers import AutoTokenizer
from app.core.config import settings
from ollama import AsyncClient
from docling.document_converter import DocumentConverter
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions
from docling.document_converter import PdfFormatOption
from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from docling.chunking import HybridChunker
from openai import AsyncOpenAI
from app.services.keyword_extractor import KeywordExtractor

class UtilsProvider(Provider):
    """
    Provider –¥–ª—è —É—Ç–∏–ª–∏—Ç: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, Qdrant, Docling, chunking.
    –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤–º–µ—Å—Ç–µ.
    """
    scope = Scope.APP
    
    utils = provide_all(
        KeywordExtractor,
    )
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
    EMBEDDING_MODEL = "intfloat/e5-base-v2"
    EMBEDDING_DIMENSION = 768  # e5-base-v2 –∏–º–µ–µ—Ç 768 –∏–∑–º–µ—Ä–µ–Ω–∏—è
    MAX_CHUNK_TOKENS = 512  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
    CHUNK_OVERLAP_TOKENS = 64  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    @provide
    def provide_sentence_transformer(self) -> SentenceTransformer:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        intfloat/e5-large-v2 - –æ–¥–Ω–∞ –∏–∑ –ª—É—á—à–∏—Ö open-source –º–æ–¥–µ–ª–µ–π.
        """
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.EMBEDDING_MODEL}")
        return SentenceTransformer(self.EMBEDDING_MODEL)
    
    @provide
    def provide_qdrant_client(self) -> AsyncQdrantClient:
        """–°–æ–∑–¥–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è Qdrant –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
        print(f"üîÑ –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant: {settings.QDRANT_HOST}:{settings.QDRANT_PORT}")
        return AsyncQdrantClient(
            host=settings.QDRANT_HOST,
            port=settings.QDRANT_PORT,
        )
   
    @provide
    def provide_ollama_client(self) -> AsyncClient:
        """–°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama LLM."""
        userpass = "goodman:password4ollama"
        auth = base64.b64encode(userpass.encode()).decode()
        client = AsyncClient(
            host="https://ollama.technocrats.uz",
            headers={"Authorization": f"Basic {auth}"}
        )
        print(f"üîÑ Ollama –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: https://ollama.technocrats.uz")
        return client
    
    @provide
    def provide_accelerator_options(self) -> AcceleratorOptions:
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç accelerator –¥–ª—è Docling.
        AUTO –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π (CUDA > MPS > CPU).
        """
        return AcceleratorOptions(
            num_threads=8,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è CPU –æ–ø–µ—Ä–∞—Ü–∏–π
            device=AcceleratorDevice.AUTO  # –ê–≤—Ç–æ–≤—ã–±–æ—Ä: CUDA > MPS > CPU
        )
    
    @provide
    def provide_pdf_pipeline_options(self) -> PdfPipelineOptions:
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∏ pipeline –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF.
        –í–∫–ª—é—á–∞–µ–º OCR –∏ table structure –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
        """
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True  # –í–∫–ª—é—á–∞–µ–º OCR –¥–ª—è –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF
        pipeline_options.do_table_structure = True  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü
        pipeline_options.table_structure_options.do_cell_matching = True

        ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True)
        pipeline_options.ocr_options = ocr_options

        return pipeline_options
    
    @provide
    def provide_document_converter(self, pipeline_options: PdfPipelineOptions) -> DocumentConverter:
        """
        –°–æ–∑–¥–∞–µ—Ç DocumentConverter —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç accelerator –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π PDF pipeline.
        """
        print(f"üîÑ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Docling DocumentConverter —Å accelerator: {pipeline_options.accelerator_options.device}")
        
        converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options,
                )
            }
        )
        return converter
    
    @provide
    def provide_huggingface_tokenizer(self) -> HuggingFaceTokenizer:
        """
        –°–æ–∑–¥–∞–µ—Ç tokenizer –¥–ª—è HybridChunker.
        –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—É –∂–µ –º–æ–¥–µ–ª—å —á—Ç–æ –∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤!
        """
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º tokenizer: {self.EMBEDDING_MODEL}")
        return HuggingFaceTokenizer(
            tokenizer=AutoTokenizer.from_pretrained(self.EMBEDDING_MODEL),
            max_tokens=self.MAX_CHUNK_TOKENS  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ
        )
    
    @provide
    def provide_docling_chunker(self, tokenizer: HuggingFaceTokenizer) -> HybridChunker:
        """
        –°–æ–∑–¥–∞–µ—Ç HybridChunker –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        HybridChunker:
        - –£–≤–∞–∂–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã)
        - –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫ —á–∞–Ω–∫–∞–º
        - –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã, –∞ –Ω–µ —Å–∏–º–≤–æ–ª—ã
        - –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ (merge_peers=True)
        """
        print(f"üîÑ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º HybridChunker: max_tokens={self.MAX_CHUNK_TOKENS}, merge_peers=True")
        
        return HybridChunker(
            tokenizer=tokenizer,
            merge_peers=True,  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ—Å–µ–¥–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
        )
    
    
    @provide
    def provide_openai_client(self) -> AsyncOpenAI:
        """–°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è OpenAI."""
        return AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    